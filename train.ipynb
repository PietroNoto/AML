{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip custom_hopper.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "from env.custom_hopper import *\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.sac.policies import MlpPolicy\n",
    "from optimize_hyperparam import *\n",
    "from os.path import exists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Register and train source domain environment</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CustomHopper-source-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m exists(\u001b[39m\"\u001b[39m\u001b[39mSAC_source_env.zip\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     model \u001b[39m=\u001b[39m SAC\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mSAC_source_env\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLearning rate source domain: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exists' is not defined"
     ]
    }
   ],
   "source": [
    "if exists(\"SAC_source_env.zip\"):\n",
    "    model = SAC.load(\"SAC_source_env\")\n",
    "    print(f\"Learning rate source domain: {model.learning_rate}\")\n",
    "else:\n",
    "    model = SAC(MlpPolicy, env, verbose=1)\n",
    "    model.learn(total_timesteps = 50000, log_interval = 50)\n",
    "    model.save(\"SAC_source_env\")\n",
    "    \n",
    "print('State space:', env.observation_space)  # state-space\n",
    "print('Action space:', env.action_space)  # action-space\n",
    "print('Dynamics parameters:', env.get_parameters())  # masses of each link of the Hopper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Register and train target domain environment</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CustomHopper-target-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: target domain: 0.0003\n",
      "State space: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf], (11,), float64)\n",
      "Action space: Box([-1. -1. -1.], [1. 1. 1.], (3,), float32)\n",
      "Dynamics parameters: [3.53429174 3.92699082 2.71433605 5.0893801 ]\n"
     ]
    }
   ],
   "source": [
    "if exists(\"SAC_target_env.zip\"):\n",
    "    model = SAC.load(\"SAC_target_env\")\n",
    "    print(f\"Learning rate: target domain: {model.learning_rate}\")\n",
    "else:\n",
    "    model = SAC(MlpPolicy, env, verbose=1)\n",
    "    model.learn(total_timesteps = 50000, log_interval = 50)\n",
    "    model.save(\"SAC_target_env\")\n",
    "    \n",
    "print('State space:', env.observation_space)  # state-space\n",
    "print('Action space:', env.action_space)  # action-space\n",
    "print('Dynamics parameters:', env.get_parameters())  # masses of each link of the Hopper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter optimization for source domain</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 optimize_hyperparam.py --algo sac --env CustomHopper-source-v0 -n 500 -optimize --n-jobs 4 --conf-file standard_config.yml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter optimization for target domain</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CustomHopper-target-v0 ==========\n",
      "Seed: 1150878785\n",
      "Loading hyperparameters from: standard_config.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('learning_rate', 'lin_7.3e-4'),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 1 environments\n",
      "Overwriting n_timesteps with n=500\n",
      "Doing 1 intermediate evaluations for pruning based on the number of timesteps. (1 evaluation every 100k timesteps)\n",
      "Optimizing hyperparameters\n",
      "/home/pietro/.local/lib/python3.8/site-packages/optuna/samplers/_tpe/sampler.py:282: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "Sampler: tpe - Pruner: median\n",
      "\u001b[32m[I 2023-05-18 16:50:20,177]\u001b[0m A new study created in memory with name: no-name-16a3b04f-4b7d-4722-b48a-155e4da2b0b3\u001b[0m\n",
      "/home/pietro/.local/lib/python3.8/site-packages/rl_zoo3/hyperparams_opt.py:233: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
      "\u001b[32m[I 2023-05-18 16:50:22,058]\u001b[0m Trial 2 finished with value: 34.783894 and parameters: {'gamma': 0.9, 'learning_rate': 0.08192304995376866, 'batch_size': 32, 'buffer_size': 10000, 'learning_starts': 20000, 'train_freq': 32, 'tau': 0.02, 'log_std_init': -0.88828706081688, 'net_arch': 'big'}. Best is trial 2 with value: 34.783894.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:22,411]\u001b[0m Trial 3 finished with value: 97.7055622 and parameters: {'gamma': 0.99, 'learning_rate': 0.0063301972808268315, 'batch_size': 256, 'buffer_size': 1000000, 'learning_starts': 20000, 'train_freq': 256, 'tau': 0.05, 'log_std_init': 0.915990916742011, 'net_arch': 'small'}. Best is trial 3 with value: 97.7055622.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:22,825]\u001b[0m Trial 1 finished with value: 152.54747400000002 and parameters: {'gamma': 0.9999, 'learning_rate': 0.3753143028722636, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 512, 'tau': 0.02, 'log_std_init': -1.691033571090843, 'net_arch': 'small'}. Best is trial 1 with value: 152.54747400000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:23,114]\u001b[0m Trial 5 finished with value: 18.102538199999998 and parameters: {'gamma': 0.99, 'learning_rate': 1.946916325651977e-05, 'batch_size': 16, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 16, 'tau': 0.01, 'log_std_init': -1.3874758904111886, 'net_arch': 'small'}. Best is trial 1 with value: 152.54747400000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:23,879]\u001b[0m Trial 7 finished with value: 11.567569 and parameters: {'gamma': 0.9999, 'learning_rate': 9.863860506640864e-05, 'batch_size': 1024, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': 32, 'tau': 0.08, 'log_std_init': 0.4793142987402419, 'net_arch': 'medium'}. Best is trial 1 with value: 152.54747400000002.\u001b[0m\n",
      "Expected parameter loc (Tensor of shape (64, 3)) of distribution Normal(loc: torch.Size([64, 3]), scale: torch.Size([64, 3])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "============\n",
      "Sampled hyperparams:\n",
      "{'batch_size': 64,\n",
      " 'buffer_size': 10000,\n",
      " 'ent_coef': 'auto',\n",
      " 'gamma': 0.9999,\n",
      " 'gradient_steps': 32,\n",
      " 'learning_rate': 0.21407238928620087,\n",
      " 'learning_starts': 0,\n",
      " 'policy_kwargs': {'log_std_init': -1.926849401729613,\n",
      "                   'net_arch': [64, 64],\n",
      "                   'use_sde': False},\n",
      " 'target_entropy': 'auto',\n",
      " 'tau': 0.001,\n",
      " 'train_freq': 32}\n",
      "\u001b[32m[I 2023-05-18 16:50:38,168]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:38,806]\u001b[0m Trial 9 finished with value: 12.392883999999999 and parameters: {'gamma': 0.9999, 'learning_rate': 0.003953151450896363, 'batch_size': 32, 'buffer_size': 1000000, 'learning_starts': 20000, 'train_freq': 16, 'tau': 0.08, 'log_std_init': -3.204021065717096, 'net_arch': 'small'}. Best is trial 1 with value: 152.54747400000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:39,705]\u001b[0m Trial 10 finished with value: 22.9480736 and parameters: {'gamma': 0.98, 'learning_rate': 9.35154816986494e-05, 'batch_size': 256, 'buffer_size': 10000, 'learning_starts': 10000, 'train_freq': 128, 'tau': 0.02, 'log_std_init': -1.5215809941071665, 'net_arch': 'small'}. Best is trial 1 with value: 152.54747400000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:41,345]\u001b[0m Trial 11 finished with value: 214.9583228 and parameters: {'gamma': 0.9, 'learning_rate': 0.008786411964673937, 'batch_size': 16, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': 0.20580477074293668, 'net_arch': 'small'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:42,162]\u001b[0m Trial 12 finished with value: 24.324498400000003 and parameters: {'gamma': 0.9999, 'learning_rate': 0.003298834208764469, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': 512, 'tau': 0.01, 'log_std_init': -2.147643800821998, 'net_arch': 'small'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:44,068]\u001b[0m Trial 13 finished with value: 126.94133439999999 and parameters: {'gamma': 0.9, 'learning_rate': 6.148577608633421e-05, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': 0.8512285466435531, 'net_arch': 'big'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:44,648]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:45,477]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:46,801]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:47,449]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:48,391]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:49,113]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:49,907]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:51,129]\u001b[0m Trial 6 finished with value: 40.480986400000006 and parameters: {'gamma': 0.999, 'learning_rate': 0.1165869474268685, 'batch_size': 64, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 64, 'tau': 0.01, 'log_std_init': 0.9068975009458038, 'net_arch': 'small'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:52,363]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:53,736]\u001b[0m Trial 23 finished with value: 40.5941062 and parameters: {'gamma': 0.9999, 'learning_rate': 0.5333358959005059, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.02, 'log_std_init': -0.7470910812889137, 'net_arch': 'medium'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:50:54,696]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:01,879]\u001b[0m Trial 4 finished with value: 63.11240780000001 and parameters: {'gamma': 0.9, 'learning_rate': 0.00031682935620997934, 'batch_size': 32, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 16, 'tau': 0.08, 'log_std_init': 0.4438995920439597, 'net_arch': 'medium'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:03,302]\u001b[0m Trial 26 finished with value: 86.9741874 and parameters: {'gamma': 0.95, 'learning_rate': 4.3294714873105146e-05, 'batch_size': 1024, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': 0.11382596965039593, 'net_arch': 'big'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:04,735]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:06,220]\u001b[0m Trial 28 finished with value: 71.5233474 and parameters: {'gamma': 0.9999, 'learning_rate': 0.13574083067784953, 'batch_size': 16, 'buffer_size': 100000, 'learning_starts': 1000, 'train_freq': 512, 'tau': 0.02, 'log_std_init': -1.2155317401308878, 'net_arch': 'small'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:07,072]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:08,732]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:10,541]\u001b[0m Trial 31 finished with value: 57.6419806 and parameters: {'gamma': 0.9999, 'learning_rate': 4.7509034550598583e-05, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': 0.6931514543932602, 'net_arch': 'big'}. Best is trial 11 with value: 214.9583228.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:11,562]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:14,332]\u001b[0m Trial 33 finished with value: 215.09306120000002 and parameters: {'gamma': 0.9, 'learning_rate': 0.000604414114183289, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 16, 'tau': 0.01, 'log_std_init': 0.8811308119743656, 'net_arch': 'big'}. Best is trial 33 with value: 215.09306120000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:15,111]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:17,264]\u001b[0m Trial 35 finished with value: 164.5196782 and parameters: {'gamma': 0.999, 'learning_rate': 0.0042623384994361115, 'batch_size': 32, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 16, 'tau': 0.01, 'log_std_init': 0.1507100261037445, 'net_arch': 'big'}. Best is trial 33 with value: 215.09306120000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:18,192]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:18,678]\u001b[0m Trial 0 finished with value: 77.274083 and parameters: {'gamma': 0.95, 'learning_rate': 0.002592628666813629, 'batch_size': 256, 'buffer_size': 10000, 'learning_starts': 0, 'train_freq': 32, 'tau': 0.08, 'log_std_init': 0.5689354399723889, 'net_arch': 'medium'}. Best is trial 33 with value: 215.09306120000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:19,490]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:20,056]\u001b[0m Trial 21 finished with value: 66.886691 and parameters: {'gamma': 0.9, 'learning_rate': 0.031137191499273256, 'batch_size': 16, 'buffer_size': 1000000, 'learning_starts': 0, 'train_freq': 16, 'tau': 0.005, 'log_std_init': 0.9681573946788533, 'net_arch': 'small'}. Best is trial 33 with value: 215.09306120000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:20,093]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:21,238]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:21,399]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:21,705]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:22,700]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:23,111]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:23,696]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:25,302]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:25,825]\u001b[0m Trial 45 finished with value: 104.5632748 and parameters: {'gamma': 0.9, 'learning_rate': 0.00019642136885926904, 'batch_size': 128, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 16, 'tau': 0.01, 'log_std_init': 0.9314187548162273, 'net_arch': 'small'}. Best is trial 33 with value: 215.09306120000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:26,727]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:27,382]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:28,095]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:28,679]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:30,210]\u001b[0m Trial 52 finished with value: 129.5089952 and parameters: {'gamma': 0.9, 'learning_rate': 0.000736380642787468, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': 32, 'tau': 0.01, 'log_std_init': -0.6707877311403726, 'net_arch': 'big'}. Best is trial 33 with value: 215.09306120000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:30,450]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:31,812]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:33,336]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:34,115]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:34,671]\u001b[0m Trial 54 finished with value: 152.1166422 and parameters: {'gamma': 0.9, 'learning_rate': 1.1083256894323162e-05, 'batch_size': 32, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': 0.16042981927676192, 'net_arch': 'big'}. Best is trial 33 with value: 215.09306120000002.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:36,055]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:36,786]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:38,425]\u001b[0m Trial 58 finished with value: 261.1866892 and parameters: {'gamma': 0.9, 'learning_rate': 3.908930487854969e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -0.11037432424180504, 'net_arch': 'big'}. Best is trial 58 with value: 261.1866892.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:39,057]\u001b[0m Trial 61 finished with value: 167.36218739999998 and parameters: {'gamma': 0.9, 'learning_rate': 0.00010519565934481354, 'batch_size': 32, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 256, 'tau': 0.005, 'log_std_init': -0.7196183593012417, 'net_arch': 'big'}. Best is trial 58 with value: 261.1866892.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:39,235]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:40,078]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:41,070]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:42,402]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:42,899]\u001b[0m Trial 65 finished with value: 133.2213668 and parameters: {'gamma': 0.9, 'learning_rate': 4.0383796538909114e-05, 'batch_size': 256, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 256, 'tau': 0.005, 'log_std_init': -0.3235963339905749, 'net_arch': 'big'}. Best is trial 58 with value: 261.1866892.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:43,574]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:45,746]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:46,702]\u001b[0m Trial 67 finished with value: 268.72515080000005 and parameters: {'gamma': 0.9, 'learning_rate': 1.4916415744829004e-05, 'batch_size': 32, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -0.8008147258676769, 'net_arch': 'big'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:48,176]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:48,292]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:49,874]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:50,196]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:50,790]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:52,305]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:53,100]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:54,194]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:55,327]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:56,215]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:56,871]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:57,791]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:51:59,377]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:00,346]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:01,140]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:02,012]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:02,773]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:03,704]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:04,908]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:06,245]\u001b[0m Trial 25 finished with value: 98.596322 and parameters: {'gamma': 0.9, 'learning_rate': 2.5204402075902745e-05, 'batch_size': 128, 'buffer_size': 1000000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.005, 'log_std_init': 0.29884507053153786, 'net_arch': 'big'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:06,980]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:07,780]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:08,033]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:09,550]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:09,789]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:11,187]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:11,698]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:12,314]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:13,106]\u001b[0m Trial 99 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:13,647]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:14,503]\u001b[0m Trial 100 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:15,701]\u001b[0m Trial 101 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:16,905]\u001b[0m Trial 103 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:17,542]\u001b[0m Trial 102 finished with value: 184.579992 and parameters: {'gamma': 0.9, 'learning_rate': 0.03428850973909528, 'batch_size': 16, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 32, 'tau': 0.08, 'log_std_init': -0.017460833945406318, 'net_arch': 'small'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:18,530]\u001b[0m Trial 104 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:19,249]\u001b[0m Trial 106 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:20,319]\u001b[0m Trial 105 finished with value: 127.43840100000003 and parameters: {'gamma': 0.9, 'learning_rate': 0.000190978379848064, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': 256, 'tau': 0.01, 'log_std_init': -1.2386170710856304, 'net_arch': 'big'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:20,382]\u001b[0m Trial 107 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:21,309]\u001b[0m Trial 108 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:22,100]\u001b[0m Trial 110 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:22,249]\u001b[0m Trial 109 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:23,232]\u001b[0m Trial 111 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:23,320]\u001b[0m Trial 112 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:24,249]\u001b[0m Trial 113 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:24,871]\u001b[0m Trial 114 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:25,432]\u001b[0m Trial 115 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:27,056]\u001b[0m Trial 117 finished with value: 109.3413864 and parameters: {'gamma': 0.9, 'learning_rate': 0.00012020006251132731, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 10000, 'train_freq': 256, 'tau': 0.01, 'log_std_init': -1.4305735767455332, 'net_arch': 'big'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:27,559]\u001b[0m Trial 116 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:28,363]\u001b[0m Trial 118 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:28,851]\u001b[0m Trial 119 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:29,513]\u001b[0m Trial 120 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:29,570]\u001b[0m Trial 121 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:30,431]\u001b[0m Trial 122 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:31,140]\u001b[0m Trial 123 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:31,678]\u001b[0m Trial 124 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:33,819]\u001b[0m Trial 125 finished with value: 207.68287760000004 and parameters: {'gamma': 0.9, 'learning_rate': 0.0001776592945818799, 'batch_size': 32, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': 0.5475182579212077, 'net_arch': 'small'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:35,717]\u001b[0m Trial 127 finished with value: 108.4707558 and parameters: {'gamma': 0.999, 'learning_rate': 0.000322467301459289, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': 256, 'tau': 0.01, 'log_std_init': -1.175178190768039, 'net_arch': 'big'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:36,336]\u001b[0m Trial 128 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:36,922]\u001b[0m Trial 129 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:37,701]\u001b[0m Trial 130 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:38,325]\u001b[0m Trial 131 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:39,191]\u001b[0m Trial 132 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:40,189]\u001b[0m Trial 133 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:40,994]\u001b[0m Trial 134 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:41,382]\u001b[0m Trial 135 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:42,495]\u001b[0m Trial 136 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:44,481]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:45,326]\u001b[0m Trial 138 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:46,215]\u001b[0m Trial 139 finished with value: 134.4581072 and parameters: {'gamma': 0.9, 'learning_rate': 0.00022238681125305034, 'batch_size': 32, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.001, 'log_std_init': 0.4700000846915745, 'net_arch': 'small'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:46,759]\u001b[0m Trial 140 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:47,576]\u001b[0m Trial 141 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:48,123]\u001b[0m Trial 142 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:48,779]\u001b[0m Trial 143 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:49,408]\u001b[0m Trial 144 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:50,119]\u001b[0m Trial 145 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:50,992]\u001b[0m Trial 146 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:51,813]\u001b[0m Trial 147 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:52,631]\u001b[0m Trial 148 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:53,353]\u001b[0m Trial 149 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:54,176]\u001b[0m Trial 150 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:54,907]\u001b[0m Trial 151 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:55,628]\u001b[0m Trial 152 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:56,458]\u001b[0m Trial 153 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:57,298]\u001b[0m Trial 154 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:58,227]\u001b[0m Trial 155 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:58,997]\u001b[0m Trial 156 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:52:59,684]\u001b[0m Trial 157 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:00,633]\u001b[0m Trial 158 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:02,057]\u001b[0m Trial 159 finished with value: 124.34735800000001 and parameters: {'gamma': 0.9, 'learning_rate': 1.968989681558691e-05, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 10000, 'train_freq': 64, 'tau': 0.01, 'log_std_init': -0.07431502230132825, 'net_arch': 'big'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:03,019]\u001b[0m Trial 160 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:03,954]\u001b[0m Trial 137 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:04,316]\u001b[0m Trial 161 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:05,498]\u001b[0m Trial 162 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:06,032]\u001b[0m Trial 163 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:07,152]\u001b[0m Trial 165 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:07,536]\u001b[0m Trial 164 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:08,235]\u001b[0m Trial 166 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:09,145]\u001b[0m Trial 167 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:10,341]\u001b[0m Trial 168 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:11,189]\u001b[0m Trial 169 finished with value: 222.38865 and parameters: {'gamma': 0.9, 'learning_rate': 0.00018932618082929297, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 10000, 'train_freq': 256, 'tau': 0.01, 'log_std_init': -2.0961746143455002, 'net_arch': 'big'}. Best is trial 67 with value: 268.72515080000005.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:11,473]\u001b[0m Trial 170 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:12,334]\u001b[0m Trial 171 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:12,817]\u001b[0m Trial 172 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:13,395]\u001b[0m Trial 173 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:14,329]\u001b[0m Trial 174 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:14,464]\u001b[0m Trial 175 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:15,902]\u001b[0m Trial 177 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:16,147]\u001b[0m Trial 176 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:16,865]\u001b[0m Trial 179 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:17,356]\u001b[0m Trial 178 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:18,206]\u001b[0m Trial 180 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:19,000]\u001b[0m Trial 182 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:19,994]\u001b[0m Trial 183 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:21,672]\u001b[0m Trial 184 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:22,785]\u001b[0m Trial 185 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:23,338]\u001b[0m Trial 181 finished with value: 393.5363906 and parameters: {'gamma': 0.9999, 'learning_rate': 0.3305801933060239, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': 512, 'tau': 0.02, 'log_std_init': -1.0190626611671032, 'net_arch': 'small'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:23,876]\u001b[0m Trial 187 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:24,337]\u001b[0m Trial 188 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:25,398]\u001b[0m Trial 189 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:26,617]\u001b[0m Trial 190 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:27,170]\u001b[0m Trial 191 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:28,040]\u001b[0m Trial 192 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:53:28,851]\u001b[0m Trial 193 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:01,783]\u001b[0m Trial 126 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:02,337]\u001b[0m Trial 195 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:03,082]\u001b[0m Trial 196 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:04,479]\u001b[0m Trial 197 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:08,543]\u001b[0m Trial 186 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:08,994]\u001b[0m Trial 199 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:09,562]\u001b[0m Trial 200 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:10,836]\u001b[0m Trial 201 finished with value: 119.22008760000001 and parameters: {'gamma': 0.9, 'learning_rate': 3.1519693850896065e-05, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 10000, 'train_freq': 256, 'tau': 0.01, 'log_std_init': -3.436444456553369, 'net_arch': 'small'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:20,528]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:21,150]\u001b[0m Trial 203 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:21,813]\u001b[0m Trial 204 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:22,616]\u001b[0m Trial 205 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:23,814]\u001b[0m Trial 206 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:24,647]\u001b[0m Trial 207 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:25,437]\u001b[0m Trial 208 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:26,183]\u001b[0m Trial 209 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:26,722]\u001b[0m Trial 210 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:27,586]\u001b[0m Trial 211 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:28,292]\u001b[0m Trial 212 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:29,089]\u001b[0m Trial 213 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:30,167]\u001b[0m Trial 214 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:30,949]\u001b[0m Trial 215 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:31,700]\u001b[0m Trial 216 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:32,325]\u001b[0m Trial 217 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:32,818]\u001b[0m Trial 218 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:33,393]\u001b[0m Trial 219 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:34,279]\u001b[0m Trial 220 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:34,400]\u001b[0m Trial 202 finished with value: 195.6326338 and parameters: {'gamma': 0.9, 'learning_rate': 1.1928765179467451e-05, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 0, 'train_freq': 256, 'tau': 0.01, 'log_std_init': -2.260543097557446, 'net_arch': 'small'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:35,389]\u001b[0m Trial 221 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:36,903]\u001b[0m Trial 223 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:37,841]\u001b[0m Trial 224 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:48,469]\u001b[0m Trial 225 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:49,304]\u001b[0m Trial 226 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:50,120]\u001b[0m Trial 227 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:50,865]\u001b[0m Trial 228 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:51,795]\u001b[0m Trial 198 finished with value: 140.4797086 and parameters: {'gamma': 0.9, 'learning_rate': 2.2377711656917762e-05, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 0, 'train_freq': 128, 'tau': 0.01, 'log_std_init': -1.256442203358331, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:52,371]\u001b[0m Trial 229 finished with value: 125.84191860000001 and parameters: {'gamma': 0.999, 'learning_rate': 1.606604615708833e-05, 'batch_size': 32, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 256, 'tau': 0.005, 'log_std_init': -1.4928049608201563, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:53,085]\u001b[0m Trial 231 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:54,908]\u001b[0m Trial 222 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:54:55,877]\u001b[0m Trial 233 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:55:23,313]\u001b[0m Trial 232 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:55:29,805]\u001b[0m Trial 230 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:55:31,357]\u001b[0m Trial 234 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:55:32,515]\u001b[0m Trial 237 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:55:58,062]\u001b[0m Trial 236 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:55:59,132]\u001b[0m Trial 239 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:56:10,985]\u001b[0m Trial 194 finished with value: 155.39915380000002 and parameters: {'gamma': 0.9, 'learning_rate': 1.0213910050042415e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -0.22249069831904567, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:56:23,095]\u001b[0m Trial 238 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:56:28,699]\u001b[0m Trial 240 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:56:36,998]\u001b[0m Trial 235 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:57:02,664]\u001b[0m Trial 243 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:57:03,375]\u001b[0m Trial 245 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:57:03,821]\u001b[0m Trial 246 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:57:36,940]\u001b[0m Trial 244 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:57:37,639]\u001b[0m Trial 248 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:57:38,348]\u001b[0m Trial 249 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:57:50,206]\u001b[0m Trial 242 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:58:24,607]\u001b[0m Trial 251 finished with value: 141.8889744 and parameters: {'gamma': 0.9, 'learning_rate': 2.8774870928675925e-05, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 0, 'train_freq': 128, 'tau': 0.01, 'log_std_init': -0.9603863387367542, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:58:48,635]\u001b[0m Trial 252 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:58:56,279]\u001b[0m Trial 241 finished with value: 132.6157066 and parameters: {'gamma': 0.9, 'learning_rate': 0.000216911865705669, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -0.2441019033440525, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:58:56,875]\u001b[0m Trial 254 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 16:59:48,481]\u001b[0m Trial 247 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:00:29,884]\u001b[0m Trial 250 finished with value: 191.54988860000003 and parameters: {'gamma': 0.9, 'learning_rate': 1.218608111501796e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.005, 'log_std_init': 0.3803251476680828, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:01:38,440]\u001b[0m Trial 253 finished with value: 246.1544546 and parameters: {'gamma': 0.9, 'learning_rate': 1.0996572589099886e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -1.6234555083612023, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:01:47,776]\u001b[0m Trial 255 finished with value: 137.51276560000002 and parameters: {'gamma': 0.9, 'learning_rate': 1.3831836906324141e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -1.0417110579425946, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:02:29,815]\u001b[0m Trial 256 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:02:30,371]\u001b[0m Trial 260 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:03:07,853]\u001b[0m Trial 257 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:03:36,923]\u001b[0m Trial 258 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:04:00,373]\u001b[0m Trial 263 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:04:29,523]\u001b[0m Trial 259 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:04:52,947]\u001b[0m Trial 265 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:05:11,729]\u001b[0m Trial 261 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:05:14,246]\u001b[0m Trial 266 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:05:32,407]\u001b[0m Trial 267 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:05:33,079]\u001b[0m Trial 269 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:05:33,534]\u001b[0m Trial 270 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:05:49,890]\u001b[0m Trial 262 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:05:50,554]\u001b[0m Trial 272 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:05:51,242]\u001b[0m Trial 273 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:06:30,534]\u001b[0m Trial 274 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:06:39,947]\u001b[0m Trial 264 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:07:54,471]\u001b[0m Trial 268 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:08:15,611]\u001b[0m Trial 271 finished with value: 225.82317079999999 and parameters: {'gamma': 0.9, 'learning_rate': 1.1865542716729974e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.001, 'log_std_init': -2.0353959246827915, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:09:14,581]\u001b[0m Trial 275 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:09:25,039]\u001b[0m Trial 276 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:10:34,040]\u001b[0m Trial 277 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:10:54,491]\u001b[0m Trial 281 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:10:56,791]\u001b[0m Trial 278 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:11:04,979]\u001b[0m Trial 283 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:11:19,303]\u001b[0m Trial 280 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:11:28,176]\u001b[0m Trial 284 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:11:28,828]\u001b[0m Trial 286 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:11:44,952]\u001b[0m Trial 279 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:12:52,951]\u001b[0m Trial 282 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:04,076]\u001b[0m Trial 285 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:04,871]\u001b[0m Trial 290 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:14,267]\u001b[0m Trial 287 finished with value: 163.2819892 and parameters: {'gamma': 0.9, 'learning_rate': 1.068077231195795e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -1.1241022254773694, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:15,576]\u001b[0m Trial 292 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:16,415]\u001b[0m Trial 293 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:16,971]\u001b[0m Trial 294 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:25,853]\u001b[0m Trial 288 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:26,447]\u001b[0m Trial 296 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:41,953]\u001b[0m Trial 291 finished with value: 173.26012740000002 and parameters: {'gamma': 0.9, 'learning_rate': 1.8774908123839673e-05, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 0, 'train_freq': 128, 'tau': 0.01, 'log_std_init': -1.4210971003723563, 'net_arch': 'medium'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:42,399]\u001b[0m Trial 298 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:14:42,951]\u001b[0m Trial 299 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:15:32,997]\u001b[0m Trial 289 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:15:59,995]\u001b[0m Trial 301 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:06,150]\u001b[0m Trial 302 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:48,187]\u001b[0m Trial 303 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:48,508]\u001b[0m Trial 304 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:49,672]\u001b[0m Trial 305 finished with value: 216.3662698 and parameters: {'gamma': 0.9, 'learning_rate': 0.002988564708833834, 'batch_size': 256, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 256, 'tau': 0.005, 'log_std_init': -1.040601866558473, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:50,386]\u001b[0m Trial 306 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:50,843]\u001b[0m Trial 307 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:51,645]\u001b[0m Trial 308 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:54,565]\u001b[0m Trial 295 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:55,081]\u001b[0m Trial 310 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:55,635]\u001b[0m Trial 311 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 17:16:56,114]\u001b[0m Trial 312 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:30,729]\u001b[0m Trial 313 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:32,033]\u001b[0m Trial 314 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:32,829]\u001b[0m Trial 315 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:33,759]\u001b[0m Trial 316 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:39,896]\u001b[0m Trial 297 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:41,958]\u001b[0m Trial 309 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:49,877]\u001b[0m Trial 317 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:50,437]\u001b[0m Trial 320 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:51,457]\u001b[0m Trial 321 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:55,924]\u001b[0m Trial 300 finished with value: 169.9881624 and parameters: {'gamma': 0.9, 'learning_rate': 3.041865012808428e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.05, 'log_std_init': -1.6816552654996073, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:23:56,649]\u001b[0m Trial 323 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:24:07,944]\u001b[0m Trial 322 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:24:08,616]\u001b[0m Trial 325 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:24:11,002]\u001b[0m Trial 319 finished with value: 225.86526899999998 and parameters: {'gamma': 0.9, 'learning_rate': 1.1896920382251611e-05, 'batch_size': 16, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.05, 'log_std_init': -0.40398566180396717, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:24:11,958]\u001b[0m Trial 324 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:24:29,315]\u001b[0m Trial 327 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:25:43,378]\u001b[0m Trial 318 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:26:02,890]\u001b[0m Trial 330 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:26:21,053]\u001b[0m Trial 331 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:26:44,449]\u001b[0m Trial 332 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:26:48,565]\u001b[0m Trial 326 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:26:54,210]\u001b[0m Trial 328 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:10,202]\u001b[0m Trial 329 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:11,660]\u001b[0m Trial 336 finished with value: 191.2791414 and parameters: {'gamma': 0.95, 'learning_rate': 0.00017849152788508937, 'batch_size': 32, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -1.8440459736444832, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:13,137]\u001b[0m Trial 337 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:14,409]\u001b[0m Trial 338 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:15,165]\u001b[0m Trial 339 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:16,561]\u001b[0m Trial 340 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:18,381]\u001b[0m Trial 335 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:37,989]\u001b[0m Trial 341 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:27:39,509]\u001b[0m Trial 342 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:05,377]\u001b[0m Trial 344 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:06,791]\u001b[0m Trial 345 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:09,023]\u001b[0m Trial 343 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:09,928]\u001b[0m Trial 347 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:10,603]\u001b[0m Trial 348 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:11,256]\u001b[0m Trial 349 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:11,961]\u001b[0m Trial 350 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:12,791]\u001b[0m Trial 351 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:13,490]\u001b[0m Trial 352 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:14,284]\u001b[0m Trial 353 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:14,852]\u001b[0m Trial 354 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:15,800]\u001b[0m Trial 355 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:16,297]\u001b[0m Trial 356 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:30,846]\u001b[0m Trial 346 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:31,315]\u001b[0m Trial 358 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:31,828]\u001b[0m Trial 359 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:38,994]\u001b[0m Trial 360 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:39,720]\u001b[0m Trial 361 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:40,487]\u001b[0m Trial 362 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:41,116]\u001b[0m Trial 363 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:41,997]\u001b[0m Trial 357 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:28:42,662]\u001b[0m Trial 365 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:10,426]\u001b[0m Trial 364 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:11,509]\u001b[0m Trial 333 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:12,130]\u001b[0m Trial 368 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:12,537]\u001b[0m Trial 367 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:13,381]\u001b[0m Trial 370 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:15,618]\u001b[0m Trial 334 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:15,985]\u001b[0m Trial 372 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:16,699]\u001b[0m Trial 373 finished with value: 144.5183928 and parameters: {'gamma': 0.9999, 'learning_rate': 0.7270718759054097, 'batch_size': 2048, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': 512, 'tau': 0.08, 'log_std_init': -0.3361384783519801, 'net_arch': 'small'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:40,498]\u001b[0m Trial 371 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:41,058]\u001b[0m Trial 375 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:41,577]\u001b[0m Trial 376 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:55,986]\u001b[0m Trial 374 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:56,441]\u001b[0m Trial 378 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:57,045]\u001b[0m Trial 379 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:57,542]\u001b[0m Trial 380 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:58,628]\u001b[0m Trial 381 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:59,549]\u001b[0m Trial 377 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:29:59,920]\u001b[0m Trial 382 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:00,317]\u001b[0m Trial 383 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:00,852]\u001b[0m Trial 385 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:07,379]\u001b[0m Trial 386 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:30,650]\u001b[0m Trial 384 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:50,168]\u001b[0m Trial 388 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:50,535]\u001b[0m Trial 389 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:51,257]\u001b[0m Trial 390 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:51,983]\u001b[0m Trial 391 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:30:52,527]\u001b[0m Trial 392 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:08,732]\u001b[0m Trial 366 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:09,333]\u001b[0m Trial 394 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:16,772]\u001b[0m Trial 393 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:17,370]\u001b[0m Trial 396 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:18,932]\u001b[0m Trial 397 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:20,101]\u001b[0m Trial 398 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:20,862]\u001b[0m Trial 399 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:21,389]\u001b[0m Trial 400 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:22,279]\u001b[0m Trial 401 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:22,992]\u001b[0m Trial 402 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:23,563]\u001b[0m Trial 403 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:29,938]\u001b[0m Trial 395 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:37,524]\u001b[0m Trial 369 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:37,979]\u001b[0m Trial 406 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:38,684]\u001b[0m Trial 407 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:39,360]\u001b[0m Trial 408 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:57,258]\u001b[0m Trial 409 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:57,957]\u001b[0m Trial 410 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:58,590]\u001b[0m Trial 411 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:59,098]\u001b[0m Trial 412 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:31:59,810]\u001b[0m Trial 413 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:00,201]\u001b[0m Trial 414 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:01,038]\u001b[0m Trial 415 finished with value: 144.50979279999996 and parameters: {'gamma': 0.9, 'learning_rate': 0.13026884991715543, 'batch_size': 16, 'buffer_size': 1000000, 'learning_starts': 1000, 'train_freq': 32, 'tau': 0.08, 'log_std_init': 0.30548133304531805, 'net_arch': 'small'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:01,543]\u001b[0m Trial 416 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:01,973]\u001b[0m Trial 417 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:02,472]\u001b[0m Trial 418 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:03,388]\u001b[0m Trial 419 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:22,214]\u001b[0m Trial 405 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:22,686]\u001b[0m Trial 421 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:27,273]\u001b[0m Trial 422 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:28,537]\u001b[0m Trial 423 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:29,278]\u001b[0m Trial 424 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:30,034]\u001b[0m Trial 425 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:41,777]\u001b[0m Trial 387 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:42,528]\u001b[0m Trial 427 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:43,466]\u001b[0m Trial 428 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:54,635]\u001b[0m Trial 426 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:55,277]\u001b[0m Trial 430 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:57,002]\u001b[0m Trial 431 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:32:57,720]\u001b[0m Trial 432 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:33:17,851]\u001b[0m Trial 429 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:33:18,727]\u001b[0m Trial 434 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:33:19,298]\u001b[0m Trial 435 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:33:19,704]\u001b[0m Trial 436 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:33:21,298]\u001b[0m Trial 437 finished with value: 177.2404218 and parameters: {'gamma': 0.98, 'learning_rate': 0.002144691482768011, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 10000, 'train_freq': 256, 'tau': 0.01, 'log_std_init': -2.6758625472961173, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:33:22,055]\u001b[0m Trial 438 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:33:30,551]\u001b[0m Trial 420 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:10,237]\u001b[0m Trial 404 finished with value: 155.68212279999997 and parameters: {'gamma': 0.9, 'learning_rate': 1.1147486725425052e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 16, 'tau': 0.01, 'log_std_init': 0.8530757521524562, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:10,712]\u001b[0m Trial 441 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:16,943]\u001b[0m Trial 440 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:18,198]\u001b[0m Trial 443 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:35,021]\u001b[0m Trial 442 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:35,594]\u001b[0m Trial 444 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:36,231]\u001b[0m Trial 446 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:51,015]\u001b[0m Trial 445 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:51,496]\u001b[0m Trial 448 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:51,831]\u001b[0m Trial 449 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:52,218]\u001b[0m Trial 450 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:55,408]\u001b[0m Trial 433 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:56,579]\u001b[0m Trial 452 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:34:56,979]\u001b[0m Trial 453 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:35:32,923]\u001b[0m Trial 454 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:35:33,652]\u001b[0m Trial 455 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:35:34,233]\u001b[0m Trial 456 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:35:50,491]\u001b[0m Trial 439 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:35:50,984]\u001b[0m Trial 458 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:37:12,495]\u001b[0m Trial 447 finished with value: 142.02564539999997 and parameters: {'gamma': 0.995, 'learning_rate': 1.5488920093766596e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 4, 'tau': 0.02, 'log_std_init': -2.6463260055163005, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:37:25,193]\u001b[0m Trial 451 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:37:26,666]\u001b[0m Trial 461 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:38:12,593]\u001b[0m Trial 457 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:38:12,993]\u001b[0m Trial 463 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:38:39,142]\u001b[0m Trial 459 finished with value: 173.88504399999997 and parameters: {'gamma': 0.999, 'learning_rate': 5.919925150839224e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 64, 'tau': 0.001, 'log_std_init': -2.438641702514345, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:39:33,449]\u001b[0m Trial 462 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:39:42,980]\u001b[0m Trial 466 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:39:54,142]\u001b[0m Trial 460 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:40:55,087]\u001b[0m Trial 464 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:41:20,549]\u001b[0m Trial 468 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:41:20,996]\u001b[0m Trial 470 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:41:22,517]\u001b[0m Trial 465 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:41:43,293]\u001b[0m Trial 472 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:41:44,283]\u001b[0m Trial 473 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:42:23,163]\u001b[0m Trial 467 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:43:20,918]\u001b[0m Trial 469 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:43:53,837]\u001b[0m Trial 476 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:43:54,221]\u001b[0m Trial 477 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:43:59,688]\u001b[0m Trial 471 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:00,712]\u001b[0m Trial 479 finished with value: 195.0022868 and parameters: {'gamma': 0.99, 'learning_rate': 1.1198640537396632e-05, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.05, 'log_std_init': -1.2851575352456326, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:01,334]\u001b[0m Trial 480 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:31,472]\u001b[0m Trial 474 finished with value: 147.01529019999998 and parameters: {'gamma': 0.999, 'learning_rate': 0.00017545048369238593, 'batch_size': 2048, 'buffer_size': 100000, 'learning_starts': 0, 'train_freq': 64, 'tau': 0.001, 'log_std_init': -1.2357530764913458, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:31,905]\u001b[0m Trial 482 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:32,743]\u001b[0m Trial 483 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:33,529]\u001b[0m Trial 484 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:34,355]\u001b[0m Trial 485 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:42,764]\u001b[0m Trial 486 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:43,515]\u001b[0m Trial 487 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:44:49,768]\u001b[0m Trial 475 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:45:13,726]\u001b[0m Trial 481 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:45:14,359]\u001b[0m Trial 490 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:46:34,703]\u001b[0m Trial 478 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:46:48,884]\u001b[0m Trial 488 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:46:49,491]\u001b[0m Trial 493 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:47:25,400]\u001b[0m Trial 494 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:47:25,510]\u001b[0m Trial 489 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:47:26,658]\u001b[0m Trial 496 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:47:27,444]\u001b[0m Trial 497 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:47:33,059]\u001b[0m Trial 491 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:47:34,979]\u001b[0m Trial 499 finished with value: 201.2166418 and parameters: {'gamma': 0.95, 'learning_rate': 7.481647144729479e-05, 'batch_size': 128, 'buffer_size': 10000, 'learning_starts': 1000, 'train_freq': 4, 'tau': 0.005, 'log_std_init': -1.7782666708116839, 'net_arch': 'big'}. Best is trial 181 with value: 393.5363906.\u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:47:40,199]\u001b[0m Trial 495 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:47:59,108]\u001b[0m Trial 498 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-05-18 18:48:09,669]\u001b[0m Trial 492 pruned. \u001b[0m\n",
      "Number of finished trials:  500\n",
      "Best trial:\n",
      "Value:  393.5363906\n",
      "Params: \n",
      "    gamma: 0.9999\n",
      "    learning_rate: 0.3305801933060239\n",
      "    batch_size: 2048\n",
      "    buffer_size: 1000000\n",
      "    learning_starts: 10000\n",
      "    train_freq: 512\n",
      "    tau: 0.02\n",
      "    log_std_init: -1.0190626611671032\n",
      "    net_arch: small\n",
      "Writing report to logs/sac/report_CustomHopper-target-v0_500-trials-500-tpe-median_1684428489\n"
     ]
    }
   ],
   "source": [
    "!python3 optimize_hyperparam.py --algo sac --env CustomHopper-target-v0 -n 500 -optimize --n-jobs 4 --conf-file standard_config.yml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
